SLURM_SUBMIT_DIR=/home/jihyunlee/bert-extract-qa
CUDA_HOME=/opt/ohpc/pub/apps/cuda-11.2
CUDA_VISIBLE_DEVICES=7
CUDA_VERSION=11.2
Start
conda PATH 
source  /home/jihyunlee/anaconda3/etc/profile.d/conda.sh
conda activate QA 
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Reusing dataset mrqa (/home/jihyunlee/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19)
Load processed data
preprocessing data...
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:03<00:07,  3.54s/it]100%|██████████| 3/3 [00:04<00:00,  1.09s/it]100%|██████████| 3/3 [00:04<00:00,  1.34s/it]
Reusing dataset mrqa (/home/jihyunlee/.cache/huggingface/datasets/mrqa/plain_text/1.1.0/1f2cf5ac32b43f864e6f91d384057a16b69b7d13ba9bcaa200ac277c90938d19)
preprocessing mrqa data
Encoding dataset (it will takes some time)
add token position
Load processed data
preprocessing data...
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:00,  4.63it/s]100%|██████████| 3/3 [00:00<00:00, 13.19it/s]
preprocessing mrqa data
Encoding dataset (it will takes some time)
add token position
Epoch : 0
Traceback (most recent call last):
  File "/home/jihyunlee/bert-extract-qa/main.py", line 111, in <module>
    main()
  File "/home/jihyunlee/bert-extract-qa/main.py", line 75, in main
    train(model, train_loader, optimizer, device)
  File "/home/jihyunlee/bert-extract-qa/trainer.py", line 13, in train
    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1770, in forward
    outputs = self.bert(
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 966, in forward
    encoder_outputs = self.encoder(
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 567, in forward
    layer_outputs = layer_module(
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 455, in forward
    self_attention_outputs = self.attention(
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 386, in forward
    self_outputs = self.self(
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/jihyunlee/anaconda3/envs/QA/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 290, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 10.76 GiB total capacity; 9.54 GiB already allocated; 188.56 MiB free; 9.58 GiB reserved in total by PyTorch)
 conda deactivate QA 
deactivate does not accept arguments
remainder_args: ['QA']

             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
             47999    2080ti  extract jihyunle  R      10:34      1 n5
##### END #####
